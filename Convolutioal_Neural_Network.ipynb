{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/hU+yu2TTXJPAjcm/2i6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonMullen/Data-Science-Projects/blob/main/Convolutioal_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Convolution"
      ],
      "metadata": {
        "id": "AMnzJADP2udR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8O3xHzw2cfO"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "italicized text# The Convolutional Class"
      ],
      "metadata": {
        "id": "cnHlSJ8Y20BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install upgrade -ignore-installed tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J_HGlSm2zp7",
        "outputId": "43113739-4227-4248-ffde-e87bb9f01298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: The index url \"gnore-installed\" seems invalid, please provide a scheme.\u001b[0m\n",
            "Looking in indexes: gnore-installed, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[33mWARNING: Location 'gnore-installed/upgrade/' is ignored: it is either a non-existing path or lacks a specific scheme.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for upgrade\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementation of forwards and backwards propogation\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "from keras.layers import Layer\n",
        "\n",
        "#The creation of a 4D convolutional Layer\n",
        "class Convolutional(Layer):\n",
        "    def __init__(self, input_shape, kernel_size, depth): #Constructor takes in three parameters\n",
        "        input_depth, input_height, input_width = input_shape #unpacking the input shape\n",
        "        self.depth = depth\n",
        "\n",
        "        #Had to slightly edit the name\n",
        "        self._input_shape = input_shape\n",
        "\n",
        "\n",
        "        self.input_depth = input_depth\n",
        "        #Had to edit this one too because of the conflicting names\n",
        "        self._output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1) #Computing the output shape\n",
        "        \n",
        "        #There is the depth which is the number of kernals, then there is the height and the width of the output matrix\n",
        "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size) #Computes the shape of the Kernals. \n",
        "        #This specific Kernal is four dimensional, there are multiple Kernels and each is a 3D block.\n",
        "        self.kernels = np.random.randn(*self.kernels_shape) #Randomly initializing each Kernal\n",
        "        \n",
        "        #Need to fix for some reason.\n",
        "        self.biases = np.random.randn(*self._output_shape) #Randomly initializing each Kernal\n",
        "        \n",
        "        #Implementing Feedforward(Propogation)\n",
        "                #So, given the Derivative of E with respect to the output, we want to compute the derivate of E with respect to the Kernal.\n",
        "\n",
        "    def forward(self, _input):\n",
        "        self._input = _input \n",
        "        self._output = np.copy(self.biases) \n",
        "        #TODO: Implement the forward method using the formauala provided in the powerpoint.\n",
        "        #You may add or remove any variables that you wish\n",
        "        for i in range(self.depth): #For loop that goes throught the output depth\n",
        "            for j in range(self.input_depth): #For loop that goes throught the input depth\n",
        "                self._output[i] += signal.correlate2d(self._input[j],self.kernels[i,j], \"valid\") #Keeps updating the output and it computs the cross correlation\n",
        "        return self._output\n",
        "    \n",
        "    #Implementing backwards(BackPropogation)\n",
        "    #Updating the Kernals and their gradients\n",
        "    #So, given the Derivative of E with respect to the output, we want to compute the derivate of E with respect to the Kernal.\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "      #Initializing empty arrays for the kernel gradient and input gradient.\n",
        "      kernels_gradient = np.zeros(self.kernels_shape)\n",
        "      input_gradient = np.zeros(self.input_shape)\n",
        "          \n",
        "          #Computing the derivative of E with respect to k,i,j\n",
        "      for i in range(self.depth):\n",
        "          for j in range(self.input_depth):\n",
        "              kernels_gradient[i,j] = signal.correlate2d(self._input[j], output_gradient[i], \"valid\") #Computing the kernal gradient\n",
        "              input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i,j], \"full\") #Computing the input gradient\n",
        "      #The Bias gradients is equal to the output_gradient\n",
        "      self.kernels -= learning_rate * kernels_gradient #updating the kernals using gradient descent..\n",
        "      self.biases -= learning_rate *output_gradient #updating the biases using gradient descent.\n",
        "      return input_gradient \n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "KXrSqYTq3NW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jjU9CR4_JQpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshaping"
      ],
      "metadata": {
        "id": "T_CR2Dsy3RCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "#The reshape layer is needed because the output layer of the convolutional layer is a 3D block.\n",
        "\n",
        "class Reshape(Layer): \n",
        "    def __init__(self,input_shape, output_shape): #inherts the parameters from the base layers class.\n",
        "        #Taking in the shape of the input and the output.\n",
        "        self._input_shape = input_shape \n",
        "        self._output_shape = output_shape\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return np.reshape(input, self._output_shape) #Reshapes the input to the output shape.\n",
        "    \n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.reshape(output_gradient, self._input_shape) #Reshapes the output to the input shape."
      ],
      "metadata": {
        "id": "10-WRqaU3NY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Cross entropy "
      ],
      "metadata": {
        "id": "QzbKnFmRU1Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Binary Cross entorpy is a good choice because it compares each theorized output to the actual output which is either zero or 1.\n",
        "#Then it penalizes the probabilities based on the distance from the expected value.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "  return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1- y_pred))\n",
        "\n",
        "def binary_cross_entropy_prime(y_true, y_pred):\n",
        "  return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
      ],
      "metadata": {
        "id": "2qcuu5OdU1CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dense Layer"
      ],
      "metadata": {
        "id": "CCTbKaLt3hdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self._weights = np.random.rand(output_size, input_size)\n",
        "        self.bias = np.random.rand(output_size, 1)\n",
        "        \n",
        "        \n",
        "    def foward(self, input):\n",
        "        self.input = input \n",
        "        return np.dot(self._weights, self.input) + self.bias\n",
        "        \n",
        "        \n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        input_gradient = np.dot(self._weights.T, output_gradient)\n",
        "        self._weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return input_gradient \n",
        "        "
      ],
      "metadata": {
        "id": "R2LyzkWA3ZJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPx2DaFb3pRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Sigmoid Activtion"
      ],
      "metadata": {
        "id": "vTSpcwQX3p6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install activation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWV5k-Pr3uvN",
        "outputId": "f274fa83-58f1-4301-8f9b-580deaf175c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: activation in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from activation) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self, activation):\n",
        "                 #activation_prime):\n",
        "        self.activation = activation\n",
        "        #self.activation_prime = activation_prime\n",
        "        \n",
        "    def foward(self,input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "    \n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.multiply(output_gradient)\n",
        "                           #self.activation_prime(self.input))\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "Vw3_jD6T3wlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.layers import Layer\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "  def __init__(self):\n",
        "    def sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    super().__init__(sigmoid)\n",
        "        #def sigmoid_prime(x):\n",
        "            #s = sigmoid(x)\n",
        "           # return s * (1 - s)\n",
        "    \n"
      ],
      "metadata": {
        "id": "7K7Rmar-3z4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Code implementation"
      ],
      "metadata": {
        "id": "v5VsJyKKYRU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "def preprocessing(x,y,limit):\n",
        "  zero_index = np.where(y == 0)[0][:limit]\n",
        "  one_index = np.where(y == 1)[0][:limit]\n",
        "  all_indices = np.hstack((zero_index, one_index))\n",
        "  all_indices = np.random.permutation(all_indices)\n",
        "  x,y = x[all_indices], y[all_indices]\n",
        "  x = x.reshape(len(x),1,28,28)\n",
        "  x = x.astype(\"float32\") / 255\n",
        "  y = np_utils.to_categorical(y)\n",
        "  y = y.reshape(len(y), 2,1)\n",
        "  return x,y\n",
        "\n",
        "  #load MNIST from server, limit to 100 images per class because the CPU hasn't been optimized.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, y_train = preprocessing(x_train, y_train, 100)\n",
        "x_test, y_test = preprocessing(x_test, y_test, 100)"
      ],
      "metadata": {
        "id": "wbo_nRWpYRCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#The Convolutional Neural Network\n",
        "network = [\n",
        "    \n",
        "  Convolutional((1,28,28),3,5),\n",
        "  Sigmoid(),\n",
        "  Reshape((5,26,26),(5*26*26,1)),\n",
        "  Dense(5*26*26,100),\n",
        "  Sigmoid(),\n",
        "  Dense(100,2),\n",
        "  Sigmoid()\n",
        "\n",
        "]\n",
        "epochs = 10\n",
        "lr = 0.1\n",
        "\n",
        "\n",
        "#def predict(network, input):\n",
        " #   output = input\n",
        "  #  for layer in network:\n",
        "   #     output = layer.forward(output)\n",
        "   # return output\n",
        "\n",
        "def train(network, loss, loss_prime, x_train, y_train,epochs = 1000, learning_rate = 0.01, verbose = True):\n",
        "    for e in range(epochs):\n",
        "        error = 0 \n",
        "        for x,y in zip(x_train, y_train):\n",
        "            #forward \n",
        "            output = x\n",
        "            for layer in network:\n",
        "              output = layer.forward(output)\n",
        "            #error \n",
        "            error += binary_cross_entropy(y, output)\n",
        "            \n",
        "          \n",
        "            #backward\n",
        "            grad = binary_cross_entropy_prime(y, output)\n",
        "            for layer in reversed(network):\n",
        "                grad = layer.backward(grad, learning_rate)\n",
        "            \n",
        "            #Todo: Perform back prop \n",
        "\n",
        "    error /= len(x_train)\n",
        "    print(f\"{e + 1}/{epochs}, error = {error}\")\n",
        "    \n",
        "#TODO: Run the test data through and print out your predictions\n",
        "for x,y in zip(x_test, y_test):\n",
        "    output = x\n",
        "    for layer in network:\n",
        "      output = layer.forward(output)\n",
        "      print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "pzAWpwAW32Xl",
        "outputId": "e499c2d7-50ee-4cd6-d81c-c7d95f8ecec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred: 2329, true: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-378-819d10053e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pred: {np.argmax(output)}, true: {np.argmax(y)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sigmoid' object has no attribute 'forward'"
          ]
        }
      ]
    }
  ]
}